image:
  repository: target/vela-worker
  tag: v0.9.0
  pullPolicy: IfNotPresent

## If you need to pull images from a private Docker image repository, pass in the name
## of a Kubernetes Secret that contains the needed secret. For more details, see:
## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
##
imagePullSecrets: []
# - name: "image-pull-secret"

## For small or experimental deployments of the Vela worker, 1 replica will suffice.
## For production cases, 2-3 are recommended. This does not grant additional parallelism,
## but does ensure that upgrades, config changes, and disruptions are handled more gracefully.
replicaCount: 1

## TODO: validate this is true for Vela
## When the worker receives a SIGTERM/SIGINT (config update, upgrade, etc), it will wait until
## all jobs that particular pod has spawned complete. It is for this reason that you'll want
## to make sure that this value is longer than your longest job.
terminationGracePeriodSeconds: 3600

nameOverride: ""
fullnameOverride: ""

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

## Add extra annotations to the Vela worker pods here.
##
podAnnotations: {}

## Add extra labels to the Vela worker pods here.
podLabels: {}

## Using topology spread constraints, you can ensure that there is at least one worker
## pod for each topology zone, e.g. one per arch for for multi-architecture clusters
## or one for each region for geographically distributed cloud-hosted clusters.
## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
topologySpreadConstraints: []
# - maxSkew: 1
#   topologyKey: "beta.kubernetes.io/arch"
#   whenUnsatisfiable: "DoNotSchedule"
#   labelSelector:
#     matchLabels:
#       "app.kubernetes.io/name": vela-worker

service:
  type: ClusterIP
  port: 3000

ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - "/"
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

## If you'd like to force the Vela worker to run on a specific node or set of nodes,
## set a selector here.
##
nodeSelector: {}

tolerations: []

affinity: {}

## If you'd like to make additional files or volumes available to the worker, declare additional
## Volumes here per the Pod spec's "volumes" section.
## Ref: https://kubernetes.io/docs/concepts/storage/volumes/
##
extraVolumes: []
## If you have declared extra volumes, mount them here, per the Pod Container's
## "volumeMounts" section.
##
extraVolumeMounts: []

## If you'd like to provide your own Kubernetes Secret object instead of passing your values
## in un-encrypted, pass in the name of a created + populated Secret in the same Namespace
## as the Vela worker. All secrets within this configmap will be mounted as environment
## variables, with each key/value mapping to a corresponding environment variable on the
## Vela worker.
##
extraSecretNamesForEnvFrom: []
# - my-vela-secrets

## This chart always creates a unique serviceAccount to add appropriate Role/RoleBinding.
serviceAccount:
  # The name is generated using the fullname template
  # Annotations to add to the service account
  annotations: {}
  # Fallback image pull secret.
  # If a pod does not have pull secrets, k8s will use the service account's pull secrets.
  # See: https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#serviceaccount-admission-controller
  # pullSecret: "your-pull-secret"

## Each namespace listed below will be configured such that the worker can run build Pods in
## it. This comes in the form of a Role and a RoleBinding. If you change env.VELA_RUNTIME_NAMESPACE
## make sure to update this list to include all
## namespaces.
rbac:
  buildNamespaces:
    - default

## The keys within the "env" map are mounted as environment variables on the Vela worker pod.
##
env:
  ## Vela Worker vars
  ## Ref: https://github.com/go-worker/server/blob/master/cmd/vela-worker/flags.go

  ## REQUIRED!
  ## Set the secret secret token that the Vela server and its workers will use
  ## to authenticate. This is commented out in order to leave you the ability to set the
  ## key via a separately provisioned secret (see existingSecretName above).
  ## This should be the same as VELA_SECRET set on the server.
  ## Usage: secret used for server <-> worker communication
  ##
  # VELA_SERVER_SECRET:

  ## REQUIRED!
  ## Defaults to the "vela-server" service that the vela-server Chart creates by default.
  ## Usage: Vela server address as a fully qualified url (<scheme>://<host>)
  ##
  VELA_SERVER_ADDR: http://vela-server

  ## This is the address that the worker will use to register with the server.
  ## Every worker needs its own worker address.
  ## Usage: Worker server address as a fully qualified url (<scheme>://<host>)
  ## TODO: Generate this? maybe tpl it?
  VELA_WORKER_ADDR: http://vela-worker:8080

  ## The worker can serve API requests (metrics, health, etc) over HTTPS.
  ## To use this, CERT and KEY should have the path to relevant TLS files.
  ##
  ## Usage: optional TLS certificate for https
  # VELA_SERVER_CERT:
  ##
  ## Usage: optional TLS certificate key
  # VELA_SERVER_CERT_KEY:

  ## REQUIRED!
  ## You need to configure the queue that the server will use to pass work to the workers.
  ##
  ## Options: redis (stubs: kafka)
  ## Usage: driver to be used for the queue
  VELA_QUEUE_DRIVER: redis
  ##
  ## Usage: fully qualified url (<scheme>://<host>) for the queue
  # VELA_QUEUE_ADDR:
  ##
  ## Usage: enables connecting to a queue cluster
  # VELA_QUEUE_CLUSTER:
  ##
  ## Usage: list of routes (channels/topics) to publish builds
  # VELA_QUEUE_ROUTES: "vela"
  ##
  ## Usage: timeout for requests that pop items off the queue
  # VELA_QUEUE_POP_TIMEOUT: "60s"

  ## Worker's executor settings
  ##
  ## Options: linux, local (stubs: darwin, windows)
  ## Usage: driver to be used for the executor
  VELA_EXECUTOR_DRIVER: linux

  ## Worker's runtime settings
  ##
  ## Options: docker, kubernetes
  ## Usage: driver to be used for the runtime
  VELA_RUNTIME_DRIVER: kubernetes
  ##
  ## Usage: path to configuration file for the runtime
  # VELA_RUNTIME_CONFIG:
  ##
  ## Determines the default Kubernetes namespace for Vela builds to run in.
  ## Usage: namespace to use for the runtime (only used by kubernetes)
  VELA_RUNTIME_NAMESPACE: default
  ##
  ## Usage: list of images allowed to run in privileged mode for the runtime
  # VELA_RUNTIME_PRIVILEGED_IMAGES: target/vela-docker
  ##
  ## Usage: list of host volumes to mount for the runtime
  # VELA_RUNTIME_VOLUMES:

  ## Other misc Worker settings

  ##
  ## Usage: set log level - options: (trace|debug|info|warn|error|fatal|panic)
  # VELA_LOG_LEVEL: info
  # EXECUTOR_LOG_LEVEL: info
  # RUNTIME_LOG_LEVEL: info
  # VELA_QUEUE_LOG_LEVEL: info
  ##
  ## Usage: format of logs to output
  # VELA_LOG_FORMAT: json
  # EXECUTOR_LOG_FORMAT: json
  # RUNTIME_LOG_FORMAT: json
  # VELA_QUEUE_LOG_FORMAT: json
  ##
  ## Usage: maximum amount of builds that can run concurrently
  # VELA_BUILD_LIMIT: "1"
  ##
  ## Usage: maximum amount of time a build can run for
  # VELA_BUILD_TIMEOUT: "30m"
